{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Deep Learning I</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Projeto 1 - Prevendo Aluguel com Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste projeto, você iremos construir um rede neural e usá-la para prever o rendimento diário de aluguel de bicicletas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando e Preparando os Dados\n",
    "\n",
    "Um passo crítico no trabalho com redes neurais é preparar os dados corretamente. Variáveis em diferentes escalas tornam difícil para a rede aprender com eficiência os pesos corretos. Abaixo, escrevemos o código para carregar e preparar os dados. Você aprenderá mais sobre isso em breve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Jave\\Seasonality\\dataset\\001410010001_001410060000002201.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando os Dados\n",
    "\n",
    "Este conjunto de dados tem o número de ciclistas para cada hora de cada dia de 1 de janeiro de 2011 a 31 de dezembro de 2012. O número de ciclistas é dividido entre casual e registrado, resumido na coluna `cnt`. Você pode ver as primeiras linhas dos dados acima.\n",
    "\n",
    "Abaixo está um gráfico que mostra o número de ciclistas durante os primeiros 10 dias no conjunto de dados. Você pode ver os aluguéis por hora. Estes dados são bastante complicados! Os fins de semana tem números mais baixos e há picos quando as pessoas andam de bicicleta para o trabalho durante a semana. Olhando para os dados acima, também temos informações sobre temperatura, umidade e velocidade do vento, tudo isso provavelmente afetando o número de ciclistas. Você estará tentando capturar tudo isso com seu modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x = 'date', y = 'quantity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis Dummy\n",
    "Temos algumas variáveis categóricas como estação, clima, mês. Para incluí-las em nosso modelo, precisamos criar variáveis simuladas binárias. Isso é simples com o Pandas graças a `get_dummies ()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_fields = ['month', 'days', 'week_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(df[each], prefix = each, drop_first = False)\n",
    "    df = pd.concat([df, dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_para_deletar = ['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(colunas_para_deletar, axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalando as Variáveis\n",
    "Para facilitar a formação da rede, padronizaremos cada uma das variáveis contínuas. Ou seja, mudaremos e escalaremos as variáveis de modo que tenham uma média zero e um desvio padrão de 1. Os fatores de escala são salvos para que possamos fazer o rollback quando usamos a rede para previsões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_numericas = ['quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in features_numericas:\n",
    "    mean, std = data[item].mean(), data[item].std()\n",
    "    scaled_features[item] = [mean, std]\n",
    "    data.loc[:, item] = (data[item] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividindo os dados em treino, teste e validação\n",
    "\n",
    "Salvaremos os últimos 21 dias dos dados para serem usados como um conjunto de testes depois de treinarmos a rede. Usaremos este conjunto para fazer previsões e compará-los com o número real de ciclistas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os últimos 21 dias\n",
    "test_data = data[15:]\n",
    "data = data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separando os dados em variáveis preditoras e variável target\n",
    "target_fields = ['quantity']\n",
    "features, targets = data.drop(target_fields, axis = 1), data[target_fields]\n",
    "test_features, test_targets = test_data.drop(target_fields, axis = 1), test_data[target_fields]\n",
    "print(features)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dividir os dados em dois conjuntos, um para treinamento e outro para validação à medida que a rede está sendo treinada. Como se trata de dados de séries temporais, treinamos em dados históricos, então tentaremos prever os dados futuros (o conjunto de validação)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mantenha os últimos 60 dias dos dados restantes como um conjunto de validação\n",
    "train_features, train_targets = features[:31], targets[:60]\n",
    "val_features, val_targets = features[31:], targets[60:]\n",
    "print(train_features)\n",
    "print(train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo a Rede Neural\n",
    "\n",
    "Abaixo iremos construir a rede. Vamos configurar o Feed Forward e os hiperparâmetros: a taxa de aprendizado, o número de unidades ocultas e o número de passadas de treinamentos (epochs).\n",
    "\n",
    "A rede possui duas camadas, uma camada oculta e uma camada de saída. A camada oculta usará a função sigmoid para ativações. A camada de saída tem apenas um nó e é usada para a regressão, a saída do nó é igual à entrada do nó. Ou seja, a função de ativação é $ f (x) = x $. Uma função que recebe o sinal de entrada e gera um sinal de saída, mas leva em consideração o limite (threshold), é chamada de função de ativação. Trabalhamos através de cada camada da nossa rede, calculando as saídas para cada neurônio. Todas as saídas de uma camada tornam-se entradas para os neurônios na próxima camada. Esse processo é chamado de * propagação direta * (Feed Forward).\n",
    "\n",
    "Usamos os pesos para propagar os sinais para a frente, da entrada para as camadas de saída em uma rede neural. Usamos os pesos para propagar também o erro de trás da saída para a rede para atualizar nossos pesos. Isso é chamado * backpropagation *.\n",
    "\n",
    "> ** Dica: ** Você precisará da derivada da função de ativação de saída ($ f (x) = x $) para a implementação de backpropagation. Se você não está familiarizado com o cálculo, esta função é equivalente à equação $ y = x $. Qual é a inclinação dessa equação? Essa é a derivada de $ f (x) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe da Rede Neural\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate, verbose = False):\n",
    "        \n",
    "        # Defina o número de nós nas camadas de entrada, oculta e de saída.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Inicializando os pesos\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
    "                                       (self.hidden_nodes, self.input_nodes))\n",
    "\n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                       (self.output_nodes, self.hidden_nodes))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        self.verbose = verbose \n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Rede Neural iniciada com:\\ninput_nodes: {}\\nhidden_nodes: {}\\noutput_nodes: {}\\nlearning_rate: {}\\n\".format(\n",
    "                self.input_nodes, self.hidden_nodes, self.output_nodes, self.lr\n",
    "            ))\n",
    "        \n",
    "        self.hidden_layer_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        self.hidden_layer_derivate_activation_function = lambda sigmoid: sigmoid * (1 - sigmoid)\n",
    "        self.output_layer_activation_function = lambda x: x \n",
    "        self.output_layer_derivate_activation_function = lambda x: 1 \n",
    "        \n",
    "        # Ativação\n",
    "        self.activation_function = self.hidden_layer_activation_function \n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        \n",
    "        # Forward pass #\n",
    "        \n",
    "        # Sinais da camada de entrada\n",
    "        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs) \n",
    "        \n",
    "        # Sinais da camada oculta\n",
    "        hidden_outputs = self.hidden_layer_activation_function(hidden_inputs) \n",
    "        \n",
    "        # Sinais na camada de saída final\n",
    "        final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs) \n",
    "        final_outputs = self.output_layer_activation_function(final_inputs)\n",
    "        return [hidden_inputs, hidden_outputs, final_inputs, final_outputs]\n",
    "    \n",
    "    def backward_pass(self, inputs, targets, hidden_inputs, hidden_outputs, final_inputs, final_outputs):\n",
    "        \n",
    "        # Backward pass #\n",
    "        output_errors = targets - final_outputs\n",
    "        output_grad = self.output_layer_derivate_activation_function(final_outputs)\n",
    "        \n",
    "        # Backpropagated error\n",
    "        \n",
    "        # Erros propagados para a camada oculta\n",
    "        hidden_errors = output_errors * output_grad * self.weights_hidden_to_output \n",
    "        hidden_grad = self.hidden_layer_derivate_activation_function(hidden_outputs)\n",
    "            \n",
    "        return [output_errors, hidden_errors, hidden_grad, output_grad]\n",
    "\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        \n",
    "        # Converta a lista de entradas para a matriz 2d\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        hidden_inputs, hidden_outputs, final_inputs, final_outputs = self.forward_pass(inputs)\n",
    "        output_errors, hidden_errors, hidden_grad, output_grad = self.backward_pass(inputs, targets, hidden_inputs, hidden_outputs, final_inputs, final_outputs)\n",
    "\n",
    "        # Atualização de pesos para saída com passo de descida de gradiente\n",
    "        self.weights_hidden_to_output += self.lr * np.dot(output_errors * output_grad, hidden_outputs.T) \n",
    "        self.weights_input_to_hidden += self.lr * np.dot(hidden_errors.T * hidden_grad, inputs.T)\n",
    " \n",
    "    def run(self, inputs_list):\n",
    "        # Executa um passo para a frente pela rede\n",
    "        inputs = np.array(inputs_list, ndmin = 2).T\n",
    "        \n",
    "        _, _, _, final_outputs = self.forward_pass(inputs)\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, Y):\n",
    "    return np.mean((y-Y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando a Rede\n",
    "\n",
    "Aqui você configurará os hiperparâmetros da rede. A estratégia é encontrar hiperparâmetros, de modo que o erro no conjunto de treinamento seja baixo, mas você não tenha overfitting. Se você treinar a rede por muito tempo ou tiver muitos nós ocultos, pode tornar-se excessivamente específico para o conjunto de treinamento e não conseguirá generalizar para o conjunto de validação. Ou seja, a perda no conjunto de validação começará a aumentar à medida que a queda do conjunto de treinamento cai.\n",
    "\n",
    "Você também usará um método conhecido como Descida de Gradiente Estocástica (SGD) para treinar a rede. A idéia é que, para cada passagem de treinamento, você pega uma amostra aleatória dos dados em vez de usar todo o conjunto de dados. Você usa muito mais passagens de treinamento do que com descida de gradiente normal, mas cada passagem é muito mais rápida. Isso acaba treinando a rede de forma mais eficiente. Você aprenderá mais sobre o SGD mais tarde.\n",
    "\n",
    "### Escolhendo número de epochs\n",
    "Este é o número de vezes que o conjunto de dados passará pela rede, cada vez que atualizar os pesos. À medida que o número de épocas aumenta, a rede se torna melhor e melhor em prever os alvos no conjunto de treinamento. Você precisará escolher épocas suficientes para treinar a rede bem, mas não demais, ou você terá overfitting.\n",
    "\n",
    "### Escolhendo a taxa de aprendizagem\n",
    "Isso reduz o tamanho das atualizações de peso. Se essa taxa for muito grande, os pesos tendem a explodir e a rede não consegue se ajustar aos dados. Uma boa escolha para começar é 0.1. Se a rede tiver problemas ao ajustar os dados, tente reduzir a taxa de aprendizado. Note-se que quanto menor for a taxa de aprendizagem, menores as etapas nas atualizações de peso e quanto maior, demora a convergir a rede neural.\n",
    "\n",
    "### Escolhendo o número de nodes ocultos\n",
    "Os nós mais ocultos que você tem, as previsões mais precisas que o modelo fará. Experimente alguns números diferentes e veja como isso afeta o desempenho. Você pode observar o dicionário de perdas para uma métrica da performance da rede. Se o número de unidades escondidas for muito baixo, então o modelo não terá espaço suficiente para aprender e, se for muito alto, há muitas opções para a direção que a aprendizagem pode levar. O truque aqui é encontrar o equilíbrio certo no número de unidades escondidas que você escolher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Configurando os hiperparametros\n",
    "epochs = 3000\n",
    "learning_rate = 0.1\n",
    "hidden_nodes = 27\n",
    "output_nodes = 1\n",
    "\n",
    "N_i = train_features.shape[1]\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate, True)\n",
    "\n",
    "losses = {'train':[], 'validation':[]}\n",
    "\n",
    "for e in range(epochs):\n",
    "    # Percorrer um lote aleatório de 128 registros do conjunto de dados de treinamento\n",
    "    batch = np.random.choice(train_features.index, size=128)\n",
    "    for record, target in zip(train_features.iloc[batch].values, \n",
    "                              train_targets.iloc[batch]['quantity']):\n",
    "        network.train(record, target)\n",
    "    \n",
    "    # Imprimir o progresso do treinamento\n",
    "    train_loss = MSE(network.run(train_features), train_targets['quantity'].values)\n",
    "    \n",
    "    val_loss = MSE(network.run(val_features), val_targets['quantity'].values)\n",
    "    \n",
    "    sys.stdout.write(\"\\rProgresso: \" + str(100 * e/float(epochs))[:4] \\\n",
    "                     + \"% ... Erro no Treinamento: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Erro na Validação: \" + str(val_loss)[:5])\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O número de epochs como 3000 é uma boa escolha em si, mas a combinação de hiperparâmetros deve ser ajustada adequadamente. Tente sintonizar todos os hiperparâmetros em paralelo. As perdas de treinamento e validação são aceitáveis. Ao observar o gráfico de perda de treinamento-validação, você não acha que há muito ruído nos dados? Isso pode ser melhorado ajustando corretamente os hiperparâmetros. Tente testar com várias combinações e observe a resposta do modelo. O número de iterações deve ser escolhido para que a perda de treinamento seja baixa e a perda de validação não aumente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Às vezes, a rede não converge quando a taxa de aprendizado é de 0.1. As etapas de atualização de peso são muito grandes com essa taxa de aprendizado e os pesos acabam por não convergir. Devido à alta taxa de aprendizado, o modelo ignora os pontos mínimos. Ao diminuir a taxa de aprendizagem, você pode obter menor perda de validação e o ruído no gráfico também diminuirá. Tente observar a resposta do modelo para valores como 0.08, 0.05, 0.01, 0.008, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses['train'], label = 'Erro no Treinamento')\n",
    "plt.plot(losses['validation'], label = 'Erro na Validação')\n",
    "plt.legend()\n",
    "plt.ylim(ymax = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo as Previsões\n",
    "\n",
    "Aqui, usamos os dados do teste para ver como a rede está modelando os dados. Se algo estiver completamente errado, certifique-se de que cada etapa da sua rede esteja implementada corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,4))\n",
    "\n",
    "mean, std = scaled_features['cnt']\n",
    "predictions = network.run(test_features)*std + mean\n",
    "ax.plot(predictions[0], label = 'Previsões')\n",
    "ax.plot((test_targets['cnt']*std + mean).values, label='Data')\n",
    "ax.set_xlim(right = len(predictions))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(df.iloc[test_data.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando os Resultados\n",
    " \n",
    "Responda estas perguntas sobre seus resultados. Quão bem o modelo prediz os dados? Onde ele falha? Por que ele falha?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> O modelo parece ter resultados bastante decentes, mas a precisão da previsão parece diminuir à medida que avançamos no tempo (a partir de 21 de dezembro). Este fenômeno provavelmente deve-se à existência de variáveis que poderiam ter acontecido entre 21 de dezembro e 31 de dezembro (parece suspeito, pois é o período das celebrações, muitos eventos inesperados podem acontecer durante esse lapso de tempo, o que poderia levar as pessoas a alugar mais bicicletas do que durante um período normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
